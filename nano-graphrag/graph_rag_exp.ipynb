{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nano_graphrag import GraphRAG, QueryParam \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nano_graphrag import GraphRAG\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수에서 Hugging Face 토큰 가져오기\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "/Users/mac/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:nano-graphrag:Load KV full_docs with 0 data\n",
      "INFO:nano-graphrag:Load KV text_chunks with 0 data\n",
      "INFO:nano-graphrag:Load KV llm_response_cache with 8 data\n",
      "INFO:nano-graphrag:Load KV community_reports with 0 data\n",
      "INFO:nano-graphrag:Loaded graph from ./llama_graph_data/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\n",
      "INFO:nano-vectordb:Load (0, 384) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './llama_graph_data/vdb_entities.json'} 0 data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# nano-graphrag에서 요구하는 래핑\n",
    "def wrap_embedding_func_with_attrs(embedding_dim, max_token_size):\n",
    "    def decorator(func):\n",
    "        func.embedding_dim = embedding_dim\n",
    "        func.max_token_size = max_token_size\n",
    "        return func\n",
    "    return decorator\n",
    "\n",
    "@wrap_embedding_func_with_attrs(embedding_dim=384, max_token_size=512)\n",
    "async def local_embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    return model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "async def my_custom_llm(prompt, system_prompt=None, history_messages=[], **kwargs):\n",
    "    response = await ollama_complete_if_cache(\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        model= 'llama3',\n",
    "        **kwargs\n",
    "    )\n",
    "    print(\"🧠 LLaMA3 응답:\", response)\n",
    "    return response\n",
    "\n",
    "# ✅ GraphRAG 객체 생성\n",
    "graph_func = GraphRAG(\n",
    "    working_dir=\"./llama_graph_data\",\n",
    "    best_model_func=my_custom_llm,             # 👉 Ollama 기반 LLM 사용\n",
    "    embedding_func=local_embedding_func        # 👉 SentenceTransformer 기반 로컬 임베딩\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:[New Docs] inserting 1 docs\n",
      "INFO:nano-graphrag:[New Chunks] inserting 4 chunks\n",
      "INFO:nano-graphrag:[Entity Extraction]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📣 [ollama_complete] 함수 호출됨!!\n",
      "📝 Full prompt length (chars): 6038\n",
      "📣 [ollama_complete] 함수 호출됨!!\n",
      "📝 Full prompt length (chars): 5943\n",
      "📣 [ollama_complete] 함수 호출됨!!\n",
      "📝 Full prompt length (chars): 5797\n",
      "📣 [ollama_complete] 함수 호출됨!!\n",
      "📝 Full prompt length (chars): 1756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LLaMA3 응답: Here is the output in the required format:\n",
      "\n",
      "## (\"entity\"<|>Master Scrooge<|>person<|>He was a bitter old man with a heart full of coldness and stone. He had no love for Christmas or its traditions, and he spent most of his time alone, away from his nephew's family.)\n",
      "## (\"entity\"<|>Schoolmaster<|>person<|>He was the master of a school where children were forced to learn. He was a stern man who liked to keep people in line.)\n",
      "## (\"entity\"<|>Ghost<|>person<|>She was a ghostly figure who appeared to Scrooge and took him on a journey through time, showing him the errors of his ways and trying to make him see the error of his ways.)\n",
      "## (\"entity\"<|>Fezziwig<|>organization<|>He was an old gentleman who used to be a kind and generous employer. He loved Christmas and liked to celebrate it with his employees.)\n",
      "## (\"relationship\"<|>Master Scrooge<|>Schoolmaster<|>The schoolmaster shook hands with Master Scrooge, which made him nervous and uneasy in his mind.<|>5)\n",
      "## (\"relationship\"<|>Master Scrooge<|>Ghost<|>The Ghost took Scrooge on a journey through time, showing him the errors of his ways and trying to make him see the error of his ways.<|>8)\n",
      "## (\"relationship\"<|>Fezziwig<|>Scrooge<|>Fezziwig was an old employer of Scrooge's, who used to be kind and generous but had become bitter and cold-hearted over time.<|>7)\n",
      "\n",
      "##<|>COMPLETE|>\n",
      "📣 [ollama_complete] 함수 호출됨!!\n",
      "📝 Full prompt length (chars): 7245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LLaMA3 응답: Here is the output:\n",
      "\n",
      "## \"entity\"<|>\"Scrooge\"<|>person<|>\"A wealthy miserly old man who has been visited by three spirits\"<|>\n",
      "\n",
      "## \"entity\"<|>\"The Spirit\"<|>event<|>\"An ethereal being who appears to Scrooge and guides him on a journey through his past, present, and future\"<|>\n",
      "\n",
      "## \"relationship\"<|>\"Scrooge\"<|>\"The Spirit\"<|>\"The Spirit visits Scrooge in the middle of the night and takes him on a journey through his past, present, and future\"<|>7\n",
      "\n",
      "## \"entity\"<|>\"Bob Cratchit\"<|>person<|>\"A kind and gentle man who is Scrooge's employee and the father of Tiny Tim\"<|>\n",
      "\n",
      "## \"relationship\"<|>\"Scrooge\"<|>\"Bob Cratchit\"<|>\"Scrooge, as his employer, has treated Bob Cratchit poorly and taken advantage of him\"<|>5\n",
      "\n",
      "## \"entity\"<|>\"Tiny Tim\"<|>person<|>\"A young boy who is the son of Bob Cratchit and has a disability that leaves him unable to walk without crutches\"<|>\n",
      "\n",
      "## \"relationship\"<|>\"Scrooge\"<|>\"Tiny Tim\"<|>\"Scrooge, despite his cold heart, is moved by the sight of Tiny Tim and begins to see the error of his ways\"<|>8\n",
      "\n",
      "## \"entity\"<|>\"London\"<|>geo<|>\"A city where Scrooge lives and works as a moneylender\"<|>\n",
      "\n",
      "## \"relationship\"<|>\"Scrooge\"<|>\"London\"<|>\"Scrooge is a part of London society, but his cold heart and miserly ways have isolated him from others\"<|>6\n",
      "\n",
      "## \"entity\"<|>\"Christmas\"<|>event<|>\"A holiday that Scrooge has always disliked and avoided, but which he begins to see as a time for joy and giving\"<|>\n",
      "\n",
      "## \"relationship\"<|>\"Scrooge\"<|>\"Christmas\"<|>\"Scrooge's attitude towards Christmas changes throughout the story, from one of disdain to one of appreciation and joy\"<|>9\n",
      "\n",
      "## COMPLETE|>\n",
      "📣 [ollama_complete] 함수 호출됨!!\n",
      "📝 Full prompt length (chars): 7751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Writing graph with 0 nodes, 0 edges\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_async/connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_async/http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_async/http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_async/http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_backends/anyio.py:32\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     26\u001b[39m exc_map = {\n\u001b[32m     27\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ReadTimeout,\n\u001b[32m     28\u001b[39m     anyio.BrokenResourceError: ReadError,\n\u001b[32m     29\u001b[39m     anyio.ClosedResourceError: ReadError,\n\u001b[32m     30\u001b[39m     anyio.EndOfStream: ReadError,\n\u001b[32m     31\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfail_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m./book.txt\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     text = f.read()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mgraph_func\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 자동으로 chunking, graph 구성\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/graphrag.py:221\u001b[39m, in \u001b[36mGraphRAG.insert\u001b[39m\u001b[34m(self, string_or_strings)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minsert\u001b[39m(\u001b[38;5;28mself\u001b[39m, string_or_strings):\n\u001b[32m    220\u001b[39m     loop = always_get_an_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mainsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring_or_strings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/graphrag.py:310\u001b[39m, in \u001b[36mGraphRAG.ainsert\u001b[39m\u001b[34m(self, string_or_strings)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# ---------- extract/summary entity and upsert to graph\u001b[39;00m\n\u001b[32m    309\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m[Entity Extraction]...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m maybe_new_kg = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.entity_extraction_func(\n\u001b[32m    311\u001b[39m     inserting_chunks,\n\u001b[32m    312\u001b[39m     knwoledge_graph_inst=\u001b[38;5;28mself\u001b[39m.chunk_entity_relation_graph,\n\u001b[32m    313\u001b[39m     entity_vdb=\u001b[38;5;28mself\u001b[39m.entities_vdb,\n\u001b[32m    314\u001b[39m     global_config=asdict(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m    315\u001b[39m     using_amazon_bedrock=\u001b[38;5;28mself\u001b[39m.using_amazon_bedrock,\n\u001b[32m    316\u001b[39m )\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m maybe_new_kg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    318\u001b[39m     logger.warning(\u001b[33m\"\u001b[39m\u001b[33mNo new entities found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/_op.py:388\u001b[39m, in \u001b[36mextract_entities\u001b[39m\u001b[34m(chunks, knwoledge_graph_inst, entity_vdb, global_config, using_amazon_bedrock)\u001b[39m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(maybe_nodes), \u001b[38;5;28mdict\u001b[39m(maybe_edges)\n\u001b[32m    387\u001b[39m \u001b[38;5;66;03m# use_llm_func is wrapped in ascynio.Semaphore, limiting max_async callings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    389\u001b[39m     *[_process_single_content(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m ordered_chunks]\n\u001b[32m    390\u001b[39m )\n\u001b[32m    391\u001b[39m \u001b[38;5;28mprint\u001b[39m()  \u001b[38;5;66;03m# clear the progress bar\u001b[39;00m\n\u001b[32m    392\u001b[39m maybe_nodes = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:349\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    351\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/_op.py:325\u001b[39m, in \u001b[36mextract_entities.<locals>._process_single_content\u001b[39m\u001b[34m(chunk_key_dp)\u001b[39m\n\u001b[32m    322\u001b[39m hint_prompt = entity_extract_prompt.format(**context_base, input_text=content)\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# print(hint_prompt[:1000])\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m final_result = \u001b[38;5;28;01mawait\u001b[39;00m use_llm_func(hint_prompt)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(final_result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    327\u001b[39m     final_result = final_result[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/_utils.py:251\u001b[39m, in \u001b[36mlimit_async_func_call.<locals>.final_decro.<locals>.wait_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(waitting_time)\n\u001b[32m    250\u001b[39m __current_size += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    252\u001b[39m __current_size -= \u001b[32m1\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mmy_custom_llm\u001b[39m\u001b[34m(prompt, system_prompt, history_messages, **kwargs)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmy_custom_llm\u001b[39m(prompt, system_prompt=\u001b[38;5;28;01mNone\u001b[39;00m, history_messages=[], **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m ollama_complete_if_cache(\n\u001b[32m     17\u001b[39m         prompt,\n\u001b[32m     18\u001b[39m         system_prompt=system_prompt,\n\u001b[32m     19\u001b[39m         history_messages=history_messages,\n\u001b[32m     20\u001b[39m         model= \u001b[33m'\u001b[39m\u001b[33mllama3\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     21\u001b[39m         **kwargs\n\u001b[32m     22\u001b[39m     )\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🧠 LLaMA3 응답:\u001b[39m\u001b[33m\"\u001b[39m, response)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/_llm.py:148\u001b[39m, in \u001b[36mollama_complete_if_cache\u001b[39m\u001b[34m(prompt, system_prompt, history_messages, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m if_cache_return \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m if_cache_return[\u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m ollama_complete(prompt, system_prompt, history_messages, **kwargs)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hashing_kv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hashing_kv.upsert({args_hash: {\u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m: response, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mollama-llama3\u001b[39m\u001b[33m\"\u001b[39m}})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/_llm.py:121\u001b[39m, in \u001b[36mollama_complete\u001b[39m\u001b[34m(prompt, system_prompt, history_messages, model, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Ollama HTTP POST 요청\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m httpx.AsyncClient(timeout=\u001b[32m180\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m client.post(\n\u001b[32m    122\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:11434/api/generate\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    123\u001b[39m         json={\n\u001b[32m    124\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    125\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: full_prompt,\n\u001b[32m    126\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# 스트리밍 꺼두면 전체 응답 반환\u001b[39;00m\n\u001b[32m    127\u001b[39m         }\n\u001b[32m    128\u001b[39m     )\n\u001b[32m    129\u001b[39m     response.raise_for_status()\n\u001b[32m    130\u001b[39m     result = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1859\u001b[39m, in \u001b[36mAsyncClient.post\u001b[39m\u001b[34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1840\u001b[39m     url: URL | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1852\u001b[39m     extensions: RequestExtensions | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1853\u001b[39m ) -> Response:\n\u001b[32m   1854\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1855\u001b[39m \u001b[33;03m    Send a `POST` request.\u001b[39;00m\n\u001b[32m   1856\u001b[39m \n\u001b[32m   1857\u001b[39m \u001b[33;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(\n\u001b[32m   1860\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1861\u001b[39m         url,\n\u001b[32m   1862\u001b[39m         content=content,\n\u001b[32m   1863\u001b[39m         data=data,\n\u001b[32m   1864\u001b[39m         files=files,\n\u001b[32m   1865\u001b[39m         json=json,\n\u001b[32m   1866\u001b[39m         params=params,\n\u001b[32m   1867\u001b[39m         headers=headers,\n\u001b[32m   1868\u001b[39m         cookies=cookies,\n\u001b[32m   1869\u001b[39m         auth=auth,\n\u001b[32m   1870\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1871\u001b[39m         timeout=timeout,\n\u001b[32m   1872\u001b[39m         extensions=extensions,\n\u001b[32m   1873\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1540\u001b[39m, in \u001b[36mAsyncClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1525\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m   1527\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m   1528\u001b[39m     method=method,\n\u001b[32m   1529\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1538\u001b[39m     extensions=extensions,\n\u001b[32m   1539\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_transports/default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    156\u001b[39m     value = typ()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LLaMA3 응답: ## (\"entity\"<|>Ebenezer Scrooge<|>person<|>He was a bitter old man with a heart full of coldness and stone. He had no love for Christmas or its traditions, and he spent most of his time alone, away from his nephew's family.)\n",
      "## (\"entity\"<|>Nephew<|>person<|>He was Scrooge's nephew, who lived with his wife and children, celebrating Christmas in a warm and loving manner.)\n",
      "## (\"entity\"<|>Sister<|>person<|>She was Scrooge's sister, who was kind and gentle, but also had a sense of mischief and playfulness.)\n",
      "## (\"entity\"<|>Postboy<|>person<|>He was a tired and worn-out postboy, who had been working all day and just wanted to rest and enjoy the Christmas celebrations.)\n",
      "## (\"entity\"<|>Dick Wilkins<|>person<|>He was Scrooge's fellow-'prentice, who was young and full of life, enjoying the festive atmosphere and celebrating with his friends.)\n",
      "## (\"entity\"<|>Warehouse<|>organization<|>It was a large warehouse where Fezziwig held Christmas celebrations for his employees, filled with music, dance, and laughter.)\n",
      "## (\"relationship\"<|>Master Scrooge<|>Nephew<|>Scrooge's nephew tried to reach out to him and bring some joy into his life, but Scrooge was too bitter and cold-hearted to respond.<|>6)\n",
      "## (\"relationship\"<|>Schoolmaster<|>Sister<|>The schoolmaster forced the sister to learn and study, which made her unhappy and rebellious.<|>4)\n",
      "\n",
      "##<|>COMPLETE|>\n",
      "⠙ Processed 1(25%) chunks,  10 entities(duplicated), 5 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LLaMA3 응답: Here are the additional entities:\n",
      "\n",
      "## \"entity\"<|>\"Ghost of Christmas Past\"<|>event<|>\"A spirit that appears to Scrooge and guides him on a journey through his past\"<|>\n",
      "\n",
      "## \"entity\"<|>\"Ghost of Christmas Present\"<|>event<|>\"A spirit that appears to Scrooge and guides him on a journey through his present, revealing the struggles of those around him\"<|>\n",
      "\n",
      "## \"entity\"<|>\"Ghost of Christmas Yet to Come\"<|>event<|>\"A spirit that appears to Scrooge and guides him on a journey through his future, showing him a possible outcome if he does not change his ways\"<|>\n",
      "\n",
      "## \"entity\"<|>\"Jacob Marley's Ghost\"<|>event<|>\"The ghost of Jacob Marley, Scrooge's former business partner who died in a state of despair due to their greedy and selfish ways\"<|>\n",
      "\n",
      "## \"entity\"<|>\"Bob Cratchit's Family\"<|>person<|>\"The family of Bob Cratchit, including his wife and children, who are struggling to make ends meet despite Scrooge's mistreatment of them\"<|>\n",
      "\n",
      "## \"entity\"<|>\"Mansion\"<|>geo<|>\"A large house that was once grand but has fallen into disrepair, symbolizing the decline of Scrooge's own life and values\"<|>\n",
      "\n",
      "## \"entity\"<|>\"School\"<|>geo<|>\"A school where a solitary child is left alone, highlighting the neglect and isolation that many children experience\"<|>\n",
      "\n",
      "## \"entity\"<|>\"Mice\"<|>person<|>\"Small creatures that live in the walls of the old mansion, representing the quiet desperation and hopelessness that can be felt by those who are forgotten or overlooked\"<|>\n",
      "\n",
      "## COMPLETE|>\n",
      "⠹ Processed 2(50%) chunks,  10 entities(duplicated), 5 relations(duplicated)\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# [3] 문서 삽입 (한 번만 실행하면 저장됨)\n",
    "with open(\"./book.txt\") as f:\n",
    "    text = f.read()\n",
    "    graph_func.insert(text)  # 자동으로 chunking, graph 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 엑사원 활용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nano_graphrag import GraphRAG\n",
    "\n",
    "# ✅ EXAONE 비동기 LLM 함수\n",
    "async def exaone_generate(prompt, system_prompt=None, history_messages=[], **kwargs):\n",
    "    async with httpx.AsyncClient(timeout=180.0) as client:\n",
    "        response = await client.post(\n",
    "            \"http://localhost:8000/generate\",  # 서버 주소 맞게 수정\n",
    "            json={\"prompt\": prompt}\n",
    "        )\n",
    "        return response.json()[\"result\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: jhgan/ko-sroberta-multitask\n"
     ]
    }
   ],
   "source": [
    "# ✅ SentenceTransformer 로드\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 안전하게 잘 작동하는 모델\n",
    "model = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
    "\n",
    "\n",
    "# ✅ nano-graphrag에 맞는 embedding function wrapper\n",
    "def wrap_embedding_func_with_attrs(embedding_dim, max_token_size):\n",
    "    def decorator(func):\n",
    "        func.embedding_dim = embedding_dim\n",
    "        func.max_token_size = max_token_size\n",
    "        return func\n",
    "    return decorator\n",
    "\n",
    "@wrap_embedding_func_with_attrs(embedding_dim=384, max_token_size=512)\n",
    "async def local_embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    return model.encode(texts, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Creating working directory ./my_graph_kor_exaone\n",
      "INFO:nano-graphrag:Load KV full_docs with 0 data\n",
      "INFO:nano-graphrag:Load KV text_chunks with 0 data\n",
      "INFO:nano-graphrag:Load KV llm_response_cache with 0 data\n",
      "INFO:nano-graphrag:Load KV community_reports with 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 384, 'metric': 'cosine', 'storage_file': './my_graph_kor_exaone/vdb_entities.json'} 0 data\n"
     ]
    }
   ],
   "source": [
    "# ✅ GraphRAG 객체 생성\n",
    "graph_func = GraphRAG(\n",
    "    working_dir=\"./my_graph_kor_exaone\",\n",
    "    best_model_func=exaone_generate,  # ⬅️ 여기만 바뀐 포인트!\n",
    "    embedding_func=local_embedding_func,\n",
    "    chunk_token_size=100000,\n",
    "    chunk_overlap_token_size=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:[New Docs] inserting 10 docs\n",
      "INFO:nano-graphrag:[New Chunks] inserting 10 chunks\n",
      "INFO:nano-graphrag:[Entity Extraction]...\n",
      "INFO:nano-graphrag:Writing graph with 0 nodes, 0 edges\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mhair_data/blog_allure.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m docs = df[\u001b[33m\"\u001b[39m\u001b[33m본문\u001b[39m\u001b[33m\"\u001b[39m].dropna().astype(\u001b[38;5;28mstr\u001b[39m).tolist()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m graph_func.ainsert(docs[:\u001b[32m10\u001b[39m])  \u001b[38;5;66;03m# ✅ 안전하게 실행됨\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/graphrag.py:328\u001b[39m, in \u001b[36mGraphRAG.ainsert\u001b[39m\u001b[34m(self, string_or_strings)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;66;03m# ---------- extract/summary entity and upsert to graph\u001b[39;00m\n\u001b[32m    327\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m[Entity Extraction]...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m maybe_new_kg = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.entity_extraction_func(\n\u001b[32m    329\u001b[39m     inserting_chunks,\n\u001b[32m    330\u001b[39m     knwoledge_graph_inst=\u001b[38;5;28mself\u001b[39m.chunk_entity_relation_graph,\n\u001b[32m    331\u001b[39m     entity_vdb=\u001b[38;5;28mself\u001b[39m.entities_vdb,\n\u001b[32m    332\u001b[39m     global_config=asdict(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m    333\u001b[39m     using_amazon_bedrock=\u001b[38;5;28mself\u001b[39m.using_amazon_bedrock,\n\u001b[32m    334\u001b[39m )\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m maybe_new_kg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m     logger.warning(\u001b[33m\"\u001b[39m\u001b[33mNo new entities found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/_op.py:451\u001b[39m, in \u001b[36mextract_entities\u001b[39m\u001b[34m(chunks, knwoledge_graph_inst, entity_vdb, global_config, using_amazon_bedrock)\u001b[39m\n\u001b[32m    448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(maybe_nodes), \u001b[38;5;28mdict\u001b[39m(maybe_edges)\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# use_llm_func is wrapped in ascynio.Semaphore, limiting max_async callings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    452\u001b[39m     *[_process_single_content(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m ordered_chunks]\n\u001b[32m    453\u001b[39m )\n\u001b[32m    454\u001b[39m \u001b[38;5;28mprint\u001b[39m()  \u001b[38;5;66;03m# clear the progress bar\u001b[39;00m\n\u001b[32m    455\u001b[39m maybe_nodes = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/_op.py:388\u001b[39m, in \u001b[36mextract_entities.<locals>._process_single_content\u001b[39m\u001b[34m(chunk_key_dp)\u001b[39m\n\u001b[32m    385\u001b[39m hint_prompt = entity_extract_prompt.format(**context_base, input_text=content)\n\u001b[32m    386\u001b[39m \u001b[38;5;66;03m# print(hint_prompt[:1000])\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m final_result = \u001b[38;5;28;01mawait\u001b[39;00m use_llm_func(hint_prompt)\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(final_result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    390\u001b[39m     final_result = final_result[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/_utils.py:271\u001b[39m, in \u001b[36mlimit_async_func_call.<locals>.final_decro.<locals>.wait_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(waitting_time)\n\u001b[32m    270\u001b[39m __current_size += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    272\u001b[39m __current_size -= \u001b[32m1\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mexaone_generate\u001b[39m\u001b[34m(prompt, system_prompt, history_messages, **kwargs)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexaone_generate\u001b[39m(prompt, system_prompt=\u001b[38;5;28;01mNone\u001b[39;00m, history_messages=[], **kwargs):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m httpx.AsyncClient(timeout=\u001b[32m180.0\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m client.post(\n\u001b[32m     10\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:8000/generate\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# 서버 주소 맞게 수정\u001b[39;00m\n\u001b[32m     11\u001b[39m             json={\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt}\n\u001b[32m     12\u001b[39m         )\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response.json()[\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1859\u001b[39m, in \u001b[36mAsyncClient.post\u001b[39m\u001b[34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1840\u001b[39m     url: URL | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1852\u001b[39m     extensions: RequestExtensions | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1853\u001b[39m ) -> Response:\n\u001b[32m   1854\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1855\u001b[39m \u001b[33;03m    Send a `POST` request.\u001b[39;00m\n\u001b[32m   1856\u001b[39m \n\u001b[32m   1857\u001b[39m \u001b[33;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(\n\u001b[32m   1860\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1861\u001b[39m         url,\n\u001b[32m   1862\u001b[39m         content=content,\n\u001b[32m   1863\u001b[39m         data=data,\n\u001b[32m   1864\u001b[39m         files=files,\n\u001b[32m   1865\u001b[39m         json=json,\n\u001b[32m   1866\u001b[39m         params=params,\n\u001b[32m   1867\u001b[39m         headers=headers,\n\u001b[32m   1868\u001b[39m         cookies=cookies,\n\u001b[32m   1869\u001b[39m         auth=auth,\n\u001b[32m   1870\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1871\u001b[39m         timeout=timeout,\n\u001b[32m   1872\u001b[39m         extensions=extensions,\n\u001b[32m   1873\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1540\u001b[39m, in \u001b[36mAsyncClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1525\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m   1527\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m   1528\u001b[39m     method=method,\n\u001b[32m   1529\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1538\u001b[39m     extensions=extensions,\n\u001b[32m   1539\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    399\u001b[39m     status_code=resp.status,\n\u001b[32m    400\u001b[39m     headers=resp.headers,\n\u001b[32m    401\u001b[39m     stream=AsyncResponseStream(resp.stream),\n\u001b[32m    402\u001b[39m     extensions=resp.extensions,\n\u001b[32m    403\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = \u001b[38;5;28;01mawait\u001b[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_async/connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_async/connection.py:76\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     72\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempted to send request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest.url.origin\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on connection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._origin\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m     )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_lock:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m             stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/httpcore/_synchronization.py:77\u001b[39m, in \u001b[36mAsyncLock.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trio_lock.acquire()\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend == \u001b[33m\"\u001b[39m\u001b[33masyncio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._anyio_lock.acquire()\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/anyio/_backends/_asyncio.py:1799\u001b[39m, in \u001b[36mLock.acquire\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fast_acquire:\n\u001b[32m   1798\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1799\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m AsyncIOBackend.cancel_shielded_checkpoint()\n\u001b[32m   1800\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m CancelledError:\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28mself\u001b[39m.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/anyio/_backends/_asyncio.py:2350\u001b[39m, in \u001b[36mAsyncIOBackend.cancel_shielded_checkpoint\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m   2347\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   2348\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcancel_shielded_checkpoint\u001b[39m(\u001b[38;5;28mcls\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2349\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m CancelScope(shield=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2350\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:640\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(delay, result)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m delay <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m __sleep0()\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    643\u001b[39m loop = events.get_running_loop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:634\u001b[39m, in \u001b[36m__sleep0\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;129m@types\u001b[39m.coroutine\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sleep0\u001b[39m():\n\u001b[32m    627\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Skip one event loop run cycle.\u001b[39;00m\n\u001b[32m    628\u001b[39m \n\u001b[32m    629\u001b[39m \u001b[33;03m    This is a private helper for 'asyncio.sleep()', used\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    632\u001b[39m \u001b[33;03m    instead of creating a Future object.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 블로그 데이터 불러오기\n",
    "df = pd.read_csv('hair_data/blog_allure.csv')\n",
    "\n",
    "docs = df[\"본문\"].dropna().astype(str).tolist()\n",
    "await graph_func.ainsert(docs[:10])  # ✅ 안전하게 실행됨\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-목표-\\n다음에 제시된 텍스트 문서와 엔티티 타입 목록을 바탕으로, 해당 타입에 해당하는 엔티티들을 모두 식별하고 이들 간의 명확한 관계를 추출하세요.\\n\\n-단계-\\n1. 엔티티 식별:\\n   문서 내에서 지정된 엔티티 타입({entity_types})에 해당하는 엔티티를 모두 찾아내고, 각각에 대해 다음 정보를 추출합니다:\\n   - entity_name: 엔티티의 이름 (모두 대문자로 표기)\\n   - entity_type: 지정된 엔티티 타입 중 하나\\n   - entity_description: 해당 엔티티의 특성과 활동에 대한 상세 설명\\n\\n   아래 형식으로 출력합니다:\\n   (\"entity\"<|><엔티티_이름><|><엔티티_타입><|><엔티티_설명>)\\n\\n2. 관계 추출:\\n   위에서 식별한 엔티티들 중 명확히 관련된 엔티티 쌍(source_entity, target_entity)을 찾고, 각각에 대해 다음 정보를 추출합니다:\\n   - source_entity: 관계의 출발점이 되는 엔티티 이름\\n   - target_entity: 관계의 도착점이 되는 엔티티 이름\\n   - relationship_description: 두 엔티티가 관련 있다고 판단한 이유를 설명하는 문장\\n   - relationship_strength: 관계의 강도를 나타내는 숫자 (1–10 사이)\\n\\n   아래 형식으로 출력합니다:\\n   (\"relationship\"<|><출발_엔티티><|><도착_엔티티><|><관계_설명><|><관계_강도>)\\n\\n3. 출력:\\n   엔티티와 관계 정보를 모두 **하나의 목록**으로 묶어 출력하며, 각 항목은 **{record_delimiter}** 로 구분합니다.\\n   출력의 마지막은 반드시 {completion_delimiter} 로 마무리해야 합니다.\\n\\n######################\\n-예시-\\n######################\\nEntity_types: [person, technology, mission, organization, location]\\nText:\\nAlex는 턱을 꽉 깨물며 분노를 참는다. 그의 옆에 있는 Taylor는 권위적인 태도로 모든 걸 지시한다. 그들 사이에는 긴장감이 흐르고, Jordan과의 협업은 마치 그 권위에 대한 조용한 저항처럼 느껴졌다.\\n\\nTaylor는 뜻밖의 행동을 한다. Jordan 옆에서 멈춰 장비를 조심스럽게 바라보며 말했다. \"이 기술이 이해된다면, 모든 것을 바꿀 수 있어요.\"\\n\\n그 말은 그들의 신념과도 맞닿아 있었다. Alex는 그 순간을 잊지 못할 것이다.\\n################\\nOutput:\\n(\"entity\"<|>\"ALEX\"<|>\"PERSON\"<|>\"Alex는 팀의 일원이며, Taylor의 권위적인 태도에 반응하며 내부 갈등을 경험하는 인물입니다.\"){record_delimiter}\\n(\"entity\"<|>\"TAYLOR\"<|>\"PERSON\"<|>\"Taylor는 권위적이고 지시적인 리더로 묘사되며, 기술 장비에 대해 경외감을 보이는 변화를 보여줍니다.\"){record_delimiter}\\n(\"entity\"<|>\"JORDAN\"<|>\"PERSON\"<|>\"Jordan은 기술 개발에 헌신적이며 Taylor와의 상호작용에서 중요한 역할을 합니다.\"){record_delimiter}\\n(\"entity\"<|>\"DEVICE\"<|>\"TECHNOLOGY\"<|>\"이 장비는 이야기의 중심 기술이며, 미래를 바꿀 수 있는 잠재력을 가집니다.\"){record_delimiter}\\n(\"relationship\"<|>\"ALEX\"<|>\"TAYLOR\"<|>\"Alex는 Taylor의 권위적인 태도에 영향을 받고 있으며, 그 행동을 관찰합니다.\"{record_delimiter}7){record_delimiter}\\n(\"relationship\"<|>\"TAYLOR\"<|>\"JORDAN\"<|>\"Taylor는 Jordan 옆에서 장비를 관찰하며 잠재적인 협력 관계를 보여줍니다.\"{record_delimiter}6){record_delimiter}\\n(\"relationship\"<|>\"TAYLOR\"<|>\"DEVICE\"<|>\"Taylor는 장비를 경외심을 가지고 바라보며 기술에 대한 인식을 드러냅니다.\"{record_delimiter}9){completion_delimiter}\\n\\n######################\\n-실제 입력-\\n######################\\nEntity_types: {entity_types}\\nText: {input_text}\\n######################\\nOutput:\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import httpx\n",
    "\n",
    "# EXAONE API 호출 함수\n",
    "async def exaone_generate(prompt):\n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        res = await client.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"exaone3.5:7.8b\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        return res.json()[\"response\"]\n",
    "\n",
    "# GraphRAG에서 사용할 LLM 함수\n",
    "async def my_custom_llm(prompt, system_prompt=None, history_messages=[], **kwargs):\n",
    "    full_prompt = \"\"\n",
    "    if system_prompt:\n",
    "        full_prompt += f\"[System]\\n{system_prompt}\\n\\n\"\n",
    "    for message in history_messages:\n",
    "        role = message.get(\"role\", \"user\")\n",
    "        content = message.get(\"content\", \"\")\n",
    "        full_prompt += f\"[{role.capitalize()}]\\n{content}\\n\\n\"\n",
    "    full_prompt += f\"[User]\\n{prompt}\"\n",
    "\n",
    "    return await exaone_generate(full_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nano_graphrag import GraphRAG\n",
    "import pandas as pd\n",
    "\n",
    "# GraphRAG 인스턴스 생성\n",
    "graph_func = GraphRAG(\n",
    "    working_dir=\"./my_graph_kor_exaone\",\n",
    "    best_model_func=my_custom_llm,         # ✅ EXAONE 기반 LLM\n",
    "    embedding_func=local_embedding_func,   # ✅ SentenceTransformer 임베딩\n",
    "    chunk_token_size=100000,\n",
    "    chunk_overlap_token_size=0,\n",
    ")\n",
    "\n",
    "# 블로그 데이터 불러오기\n",
    "df = pd.read_csv('hair_data/blog_allure.csv')\n",
    "docs = df[\"본문\"].dropna().astype(str).tolist()\n",
    "\n",
    "# 문서 삽입\n",
    "graph_func.insert(docs[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from nano_graphrag.prompt_kr import PROMPTS\n",
    "\n",
    "# 비동기 EXAONE 호출 함수\n",
    "async def exaone_generate(prompt):\n",
    "    async with httpx.AsyncClient(timeout=180.0) as client:  # 60초로 늘림\n",
    "        res = await client.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"exaone3.5:7.8b\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        return res.json()[\"response\"]\n",
    "\n",
    "\n",
    "# 프롬프트 기반 엔티티 및 관계 추출 함수\n",
    "async def extract_entities_and_relationships(input_text, entity_types):\n",
    "    prompt_template = PROMPTS[\"entity_extraction\"]\n",
    "\n",
    "    # 구성요소\n",
    "    record_delimiter = \"<R>\"\n",
    "    completion_delimiter = \"<END>\"\n",
    "\n",
    "    # 포맷팅된 프롬프트 생성\n",
    "    formatted_prompt = prompt_template.format(\n",
    "        entity_types=entity_types,\n",
    "        input_text=input_text,\n",
    "        record_delimiter=record_delimiter,\n",
    "        completion_delimiter=completion_delimiter\n",
    "    )\n",
    "\n",
    "    # EXAONE에 전달하고 결과 받기\n",
    "    output = await exaone_generate(formatted_prompt)\n",
    "\n",
    "    # 결과 파싱은 필요 시 추가\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<R>(\"entity\"<|>\"OPENAI\"<|>\"ORGANIZATION\"<|>\"OpenAI는 대규모 언어 모델 개발을 주도하는 인공지능 연구소이며, GPT 시리즈를 통해 생성형 AI 분야의 혁신을 이끌고 있습니다.\")<R>\n",
      "<R>(\"entity\"<|>\"GPT SERIES\"<|>\"TECHNOLOGY\"<|>\"GPT 시리즈는 OpenAI가 개발한 고급 언어 모델로서 생성형 AI 기술의 발전에 중추적인 역할을 합니다.\")<R>\n",
      "<R>(\"entity\"<|>\"AZURE\"<|>\"ORGANIZATION\"<|>\"Microsoft의 클라우드 플랫폼으로, 여기서 OpenAI의 AI 기술이 배포되어 기업과 개발자들에게 제공됩니다.\")<R>\n",
      "<R>(\"relationship\"<|>\"OPENAI\"<|>\"GPT SERIES\"<|>\"OpenAI는 GPT 시리즈 기술을 통해 생성형 AI 분야에서 혁신을 주도하며 핵심 기술 역할을 합니다.\"<R>9)<R>\n",
      "<R>(\"relationship\"<|>\"OPENAI\"<|>\"AZURE\"<|>\"OpenAI는 Microsoft의 Azure 플랫폼을 통해 기술 서비스를 배포함으로써 파트너십을 통해 시장 접근성을 확대하고 있습니다.\"<R>8)<R>\n",
      "<R>(\"relationship\"<|>\"GPT SERIES\"<|>\"AZURE\"<|>\"GPT 시리즈 기술은 Azure를 통해 클라우드 기반으로 제공되며, 이를 통해 사용자들에게 안정적이고 확장 가능한 AI 솔루션을 제공합니다.\"<R>8)<R>\n",
      "<R>(\"entity\"<|>\"2023\"<|>\"YEAR\"<|>\"2023년은 OpenAI와 Microsoft 간의 중요한 협력이 이루어진 해로, 특히 Azure를 통한 AI 서비스 배포가 이루어졌습니다.\")<R>\n",
      "<R>(\"relationship\"<|>\"OPENAI\"<|>\"2023\"<|>\"2023년의 사건은 OpenAI의 기술적 발전과 시장 진출 전략이 본격적으로 진행된 해를 나타냅니다.\"<R>7)<R>\n",
      "<R>(\"relationship\"<|>\"AZURE\"<|>\"2023\"<|>\"2023년에는 Azure 플랫폼을 통해 OpenAI의 AI 서비스 배포가 이루어져 해당 연도의 기술적 진보를 보여줍니다.\"<R>7)<R>\n",
      "<R>\"END\"<R>\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# 예시 입력\n",
    "text = \"\"\"\n",
    "OpenAI는 대규모 언어 모델을 개발하는 인공지능 연구소로, GPT 시리즈를 통해 생성형 AI의 한계를 확장하고 있다.\n",
    "2023년에는 Microsoft와의 협력을 통해 Azure에 AI 서비스를 배포하기도 했다.\n",
    "\"\"\"\n",
    "entity_types = [\"organization\", \"technology\", \"year\"]\n",
    "\n",
    "# 비동기 실행\n",
    "result = asyncio.run(extract_entities_and_relationships(text, entity_types))\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 활용\n",
    "- 한국어 블로그 크롤링한 것을 document로 활용하여 graphrag에 적용\n",
    "- gpt-4o-mini를 활용할 예정이며, prompt를 한국어로 바꾸는게 필요함 \n",
    "\n",
    "### 코드 구현\n",
    "1. 전체 코드 흐름에 대해 모듈화 되어 있는 것을 내 방식대로 바꾸기 \n",
    "    - 이를 위해 전체 코드에 대한 이해가 필요함\n",
    "\n",
    "2. gpt-4o-mini만 사용할 예정이며 prompt를 한국어로 바꿈 \n",
    "    - 이때, 우리의 문서에 task에 맞게 prompt를 조정하는 과정이 필요함 \n",
    "    - 한국어 document를 전처리하는 과정도 필요함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # .env 파일에 OPENAI_API_KEY가 포함되어 있어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nano_graphrag._llm import openai_complete_if_cache\n",
    "\n",
    "async def my_custom_llm(prompt, system_prompt=None, history_messages=[], **kwargs):\n",
    "    return await openai_complete_if_cache(\n",
    "        \"gpt-4o-mini\",  # 원하는 모델\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        **kwargs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: jhgan/ko-sroberta-multitask\n",
      "/Users/mac/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 안정적으로 로딩 가능한 한국어 SBERT 모델\n",
    "model_name = 'jhgan/ko-sroberta-multitask'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# nano-graphrag에서 요구하는 래핑\n",
    "def wrap_embedding_func_with_attrs(embedding_dim, max_token_size):\n",
    "    def decorator(func):\n",
    "        func.embedding_dim = embedding_dim\n",
    "        func.max_token_size = max_token_size\n",
    "        return func\n",
    "    return decorator\n",
    "\n",
    "@wrap_embedding_func_with_attrs(embedding_dim=768, max_token_size=512)\n",
    "async def local_embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    한국어 문장 리스트를 입력받아 SBERT 임베딩을 반환합니다.\n",
    "    \"\"\"\n",
    "    return model.encode(texts, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Creating working directory ./my_graph_kor\n",
      "INFO:nano-graphrag:Load KV full_docs with 0 data\n",
      "INFO:nano-graphrag:Load KV text_chunks with 0 data\n",
      "INFO:nano-graphrag:Load KV llm_response_cache with 0 data\n",
      "INFO:nano-graphrag:Load KV community_reports with 0 data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './my_graph_kor/vdb_entities.json'} 0 data\n"
     ]
    }
   ],
   "source": [
    "from nano_graphrag import GraphRAG\n",
    "\n",
    "# graph_func = GraphRAG(\n",
    "#     working_dir=\"./my_graphrag_data\",\n",
    "#     best_model_func=my_custom_llm , # 여기서 지정\n",
    "#     embedding_func=local_embedding_func,  # 여기서 지정\n",
    "# )\n",
    "graph_func = GraphRAG(\n",
    "    working_dir=\"./my_graph_kor\",\n",
    "    best_model_func=my_custom_llm,\n",
    "    embedding_func=local_embedding_func,\n",
    "\n",
    "    # ✅ 청크 사이즈를 아주 크게 설정해서 \"절대 자르지 않게\" 함\n",
    "    chunk_token_size=100000,\n",
    "    chunk_overlap_token_size=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:[New Docs] inserting 2 docs\n",
      "INFO:nano-graphrag:[New Chunks] inserting 2 chunks\n",
      "INFO:nano-graphrag:[Entity Extraction]...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1(50%) chunks,  6 entities(duplicated), 0 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Processed 2(100%) chunks,  10 entities(duplicated), 0 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Inserting 10 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.14it/s]\n",
      "INFO:nano-graphrag:[Community Report]...\n",
      "INFO:nano-graphrag:Writing graph with 10 nodes, 0 edges\n"
     ]
    },
    {
     "ename": "EmptyNetworkError",
     "evalue": "EmptyNetworkError",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyNetworkError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m df= pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mhair_data/naver_blog_per3.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m docs = df[\u001b[33m\"\u001b[39m\u001b[33m요약\u001b[39m\u001b[33m\"\u001b[39m].dropna().astype(\u001b[38;5;28mstr\u001b[39m).tolist()  \u001b[38;5;66;03m# 🔥 행 하나 = 하나의 청크로 사용됨\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mgraph_func\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/graphrag.py:222\u001b[39m, in \u001b[36mGraphRAG.insert\u001b[39m\u001b[34m(self, string_or_strings)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minsert\u001b[39m(\u001b[38;5;28mself\u001b[39m, string_or_strings):\n\u001b[32m    221\u001b[39m     loop = always_get_an_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mainsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring_or_strings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/graphrag.py:341\u001b[39m, in \u001b[36mGraphRAG.ainsert\u001b[39m\u001b[34m(self, string_or_strings)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# ---------- update clusterings of graph\u001b[39;00m\n\u001b[32m    340\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m[Community Report]...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_entity_relation_graph.clustering(\n\u001b[32m    342\u001b[39m     \u001b[38;5;28mself\u001b[39m.graph_cluster_algorithm\n\u001b[32m    343\u001b[39m )\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m generate_community_report(\n\u001b[32m    345\u001b[39m     \u001b[38;5;28mself\u001b[39m.community_reports, \u001b[38;5;28mself\u001b[39m.chunk_entity_relation_graph, asdict(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    346\u001b[39m )\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# ---------- commit upsertings and indexing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/_storage/gdb_networkx.py:138\u001b[39m, in \u001b[36mNetworkXStorage.clustering\u001b[39m\u001b[34m(self, algorithm)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m algorithm \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._clustering_algorithms:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClustering algorithm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malgorithm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not supported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._clustering_algorithms[algorithm]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_graphrag/_storage/gdb_networkx.py:204\u001b[39m, in \u001b[36mNetworkXStorage._leiden_clustering\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgraspologic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hierarchical_leiden\n\u001b[32m    203\u001b[39m graph = NetworkXStorage.stable_largest_connected_component(\u001b[38;5;28mself\u001b[39m._graph)\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m community_mapping = \u001b[43mhierarchical_leiden\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_cluster_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mglobal_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_graph_cluster_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mglobal_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgraph_cluster_seed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m node_communities: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]] = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    211\u001b[39m __levels = defaultdict(\u001b[38;5;28mset\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<@beartype(graspologic.partition.leiden.hierarchical_leiden) at 0x352010ea0>:304\u001b[39m, in \u001b[36mhierarchical_leiden\u001b[39m\u001b[34m(__beartype_object_14250901696, __beartype_object_14249558976, __beartype_getrandbits, __beartype_get_violation, __beartype_conf, __beartype_object_4381911528, __beartype_object_19314356160, __beartype_object_14249564032, __beartype_object_14264959904, __beartype_object_14264958944, __beartype_func, *args, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIworkspace/torchspace/nano-graphrag/nano_env/lib/python3.11/site-packages/graspologic/partition/leiden.py:588\u001b[39m, in \u001b[36mhierarchical_leiden\u001b[39m\u001b[34m(graph, max_cluster_size, starting_communities, extra_forced_iterations, resolution, randomness, use_modularity, random_seed, weight_attribute, is_weighted, weight_default, check_directed)\u001b[39m\n\u001b[32m    580\u001b[39m     node_count, edges = _adjacency_matrix_to_edge_list(\n\u001b[32m    581\u001b[39m         graph, identifier, check_directed, is_weighted, weight_default\n\u001b[32m    582\u001b[39m     )\n\u001b[32m    584\u001b[39m native_friendly_communities = _community_python_to_native(\n\u001b[32m    585\u001b[39m     starting_communities, identifier\n\u001b[32m    586\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m hierarchical_clusters_native = \u001b[43mgn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhierarchical_leiden\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43medges\u001b[49m\u001b[43m=\u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstarting_communities\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnative_friendly_communities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_forced_iterations\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_modularity\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_modularity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_cluster_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_cluster_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    599\u001b[39m result_partitions = HierarchicalClusters()\n\u001b[32m    600\u001b[39m all_nodes = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mEmptyNetworkError\u001b[39m: EmptyNetworkError"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd \n",
    "df= pd.read_csv('hair_data/naver_blog_per3.csv')\n",
    "docs = df[\"요약\"].dropna().astype(str).tolist()  # 🔥 행 하나 = 하나의 청크로 사용됨\n",
    "graph_func.insert(docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Revtrieved 6 communities\n",
      "INFO:nano-graphrag:Grouping to 1 groups for global search\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 봄에 어울리는 헤어스타일 추천\n",
      "\n",
      "당신이 달걀형 얼굴이라면, 봄 시즌에 잘 어울릴 여러 가지 헤어스타일이 있습니다. 아래에서 두 가지 스타일과 함께 그 특징을 살펴보겠습니다.\n",
      "\n",
      "### 1. 깻잎 머리 스타일\n",
      "\n",
      "현재 깻잎 머리 스타일이 큰 인기를 얻고 있습니다. 이 스타일은 우아함을 강조하며, 봄의 계절적 미와 잘 맞아떨어집니다. 달걀형 얼굴에 적합한 이 스타일은 얼굴의 윤곽을 부각시키고 자연스러운 매력을 더할 수 있습니다. 특히 봄에는 이 스타일이 한층 더 빛을 발할 것입니다.\n",
      "\n",
      "### 2. 히메 컷\n",
      "\n",
      "히메 컷 또한 추천해 드릴 만한 스타일입니다. 이 컷은 긴 슬릭 뱅과 뒤로 흐르는 긴 머리가 특징입니다. 다양한 얼굴형에 잘 어울리는 이 스타일은 특히 달걀형 얼굴에 flattering한 선택이 될 수 있습니다. 봄 패션과 잘 어우러져 인기가 상승할 것으로 예상되므로, 시도해 볼 만한 가치가 있습니다.\n",
      "\n",
      "### 3. 액세서리 추가\n",
      "\n",
      "헤어스타일을 한층 더 빛나게 하려면 액세서리의 역할도 중요합니다. 스프링 시즌에 잘 어울리는 헤어 클립이나 베레모 같은 액세서리를 추가하면, 스타일에 매력을 더하고 봄의 경쾌한 느낌을 강화할 수 있습니다. \n",
      "\n",
      "결론적으로, 달걀형 얼굴에는 깻잎 머리와 히메 컷이 잘 어울리며, 다양한 액세서리를 활용하면 봄의 매력을 더욱 살릴 수 있습니다. 이러한 스타일을 통해 당신의 개성을 효과적으로 표현할 수 있기를 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "# Perform global graphrag search\n",
    "print(graph_func.query(\"나는 달걀형이고, 봄에 잘 어울리는 헤어스타일을 추천해줘\"))\n",
    "\n",
    "# Perform local graphrag search (I think is better and more scalable one)\n",
    "# print(graph_func.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
