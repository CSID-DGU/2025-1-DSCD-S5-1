{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f572ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: jhgan/ko-sroberta-multitask\n",
      "INFO:nano-graphrag:Load KV full_docs with 10 data\n",
      "INFO:nano-graphrag:Load KV text_chunks with 10 data\n",
      "INFO:nano-graphrag:Load KV llm_response_cache with 43 data\n",
      "INFO:nano-graphrag:Load KV community_reports with 22 data\n",
      "INFO:nano-graphrag:Loaded graph from ./graphrag/graph_chunk_entity_relation.graphml with 732 nodes, 161 edges\n",
      "INFO:nano-vectordb:Load (724, 768) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': './graphrag/vdb_entities.json'} 724 data\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.61it/s]\n",
      "INFO:nano-graphrag:Using 20 entites, 5 communities, 38 relations, 9 text units\n",
      "INFO:nano-graphrag:Revtrieved 22 communities\n",
      "INFO:nano-graphrag:Grouping to 1 groups for global search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Local RAG Recommendation ===\n",
      "## 둥근 얼굴형과 직모의 특성\n",
      "\n",
      "둥근 얼굴형은 부드러운 윤곽선과 풍성한 볼로 특징지어지며, 이러한 얼굴형에 적합한 스타일은 얼굴의 굴곡을 자연스럽게 보완할 수 있어야 합니다. 직모와 굵은 모발을 가진 경우, 보다 정돈된 느낌의 스타일이 적합할 것입니다. 이러한 조합을 고려할 때, 관리 난이도가 쉬운 스타일을 선택하는 것이 좋습니다.\n",
      "\n",
      "## 추천 스타일: 가르마펌\n",
      "\n",
      "가르마펌은 둥근 얼굴형에 잘 어울리는 남성 스타일 중 하나입니다. 이 스타일은 깔끔하고 세련된 인상을 주며, 특히 직모와 굵은 모발에 적합합니다. **가르마펌**의 특징은 각진 얼굴형을 부드럽게 커버할 수 있기 때문에, 둥근 얼굴형에도 매우 매력적입니다. 이 스타일은 볼륨이 부족한 경우에도 효과적으로 적용할 수 있으며, 고객의 얼굴형에 맞게 개인 맞춤형 시술이 가능합니다. 특히, 면접이나 소개팅 등에서 긍정적인 인상을 줄 수 있는 스타일로 추천됩니다.\n",
      "\n",
      "## 추가 스타일링: 가일컷\n",
      "\n",
      "또 다른 추천 스타일은 **가일컷**입니다. 이 스타일은 직모에 최적화되어 있으며, 자연스럽고 단정한 느낌을 줍니다. 관리가 쉬운 스타일이며, 이마를 드러내어 성숙하고 깔끔한 분위기를 연출할 수 있습니다. 가일컷은 다양한 얼굴형에 어울리지만, 특히 둥근 얼굴형에게 더 잘 맞기 때문에 좋습니다. 가일컷은 앞머리의 길이를 조정할 수 있어, 개인의 취향에 맞게 스타일링할 수 있는 장점이 있습니다.\n",
      "\n",
      "## 관리의 용이함\n",
      "\n",
      "가르마펌과 가일컷 모두 관리가 용이하며, 바쁜 현대인들에게 특히 적합한 선택입니다. 스타일링에 필요한 제품, 예를 들어 가벼운 왁스나 헤어 스프레이를 이용하면 쉽게 원하는 스타일을 유지할 수 있습니다. 이러한 스타일들은 시간의 제약이 있는 남성들에게 매우 바람직한 옵션이 될 것입니다.\n",
      "\n",
      "위의 스타일 조합을 통해 둥근 얼굴형과 직모, 굵은 모발에서 매력적인 이미지를 연출할 수 있을 것입니다. 원하는 스타일을 선택하고 이와 적합한 관리법을 통해 자신만의 멋진 룩을 만들어 보세요!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Global RAG Recommendation ===\n",
      "## 추천 스타일\n",
      "\n",
      "당신의 둥근 얼굴형과 직모, 굵은 모발에 적합한 스타일을 고려할 때, 두 가지 스타일이 특히 추천됩니다: **가르마펌**과 **가일컷**입니다.\n",
      "\n",
      "### 가르마펌\n",
      "\n",
      "가르마펌은 각진 얼굴형과 둥근 얼굴형 모두에 적합하여, 부드러운 인상을 줄 수 있습니다. 이 스타일은 직모를 가진 남성이 자연스러운 볼륨을 추가할 수 있도록 도와줍니다. 때문에 둥근 얼굴형을 더욱 매력적으로 보이게 할 수 있는 효과가 있습니다. 특히, 관리가 용이한 장점이 있어 바쁜 일상 속에서도 편리하게 유지할 수 있습니다.\n",
      "\n",
      "### 가일컷\n",
      "\n",
      "가일컷은 굵은 모발에 잘 어울리며, 깔끔한 느낌을 주는 스타일입니다. 이 스타일은 얼굴형을 보완하고, 시원한 인상을 제공하며 유지 관리가 쉽기 때문에 당신에게 적합할 것입니다. 굵은 모발을 가진 경우, 이러한 스타일링은 더욱 뚜렷한 인상을 남기는데 기여할 수 있습니다.\n",
      "\n",
      "## 결론\n",
      "\n",
      "결론적으로, 둥근 얼굴형과 굵은 직모를 가진 당신에게는 가르마펌과 가일컷이 좋은 선택이 될 것입니다. 이 두 스타일 모두 관리가 용이하며, 각각 특유의 매력을 가져올 수 있는 옵션임을 고려하면 좋겠습니다. 스타일을 선택하기에 앞서, 각 스타일의 시각적 효과와 편의성을 잘 비교해 보시길 권장합니다.\n"
     ]
    }
   ],
   "source": [
    "from nano_graphrag import GraphRAG, QueryParam \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nano_graphrag import GraphRAG\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # .env 파일에 OPENAI_API_KEY가 포함되어 있어야 함\n",
    "from nano_graphrag._llm import openai_complete_if_cache\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nano_graphrag import GraphRAG, QueryParam  # QueryParam 별도 import\n",
    "\n",
    "\n",
    "async def my_custom_llm(prompt, system_prompt=None, history_messages=[], **kwargs):\n",
    "    return await openai_complete_if_cache(\n",
    "        \"gpt-4o-mini\",  # 원하는 모델\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "# 안정적으로 로딩 가능한 한국어 SBERT 모델\n",
    "model_name = 'jhgan/ko-sroberta-multitask'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# nano-graphrag에서 요구하는 래핑\n",
    "def wrap_embedding_func_with_attrs(embedding_dim, max_token_size):\n",
    "    def decorator(func):\n",
    "        func.embedding_dim = embedding_dim\n",
    "        func.max_token_size = max_token_size\n",
    "        return func\n",
    "    return decorator\n",
    "\n",
    "@wrap_embedding_func_with_attrs(embedding_dim=768, max_token_size=512)\n",
    "async def local_embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    한국어 문장 리스트를 입력받아 SBERT 임베딩을 반환합니다.\n",
    "    \"\"\"\n",
    "    return model.encode(texts, convert_to_numpy=True)\n",
    "graph_func = GraphRAG(\n",
    "    working_dir=\"./graphrag\",\n",
    "    best_model_func=my_custom_llm,\n",
    "    embedding_func=local_embedding_func,\n",
    "\n",
    "    # ✅ 청크 사이즈를 아주 크게 설정해서 \"절대 자르지 않게\" 함\n",
    "    chunk_token_size=100000,\n",
    "    chunk_overlap_token_size=0,\n",
    ")\n",
    "\n",
    "# 1) 질의 내용 정의\n",
    "query_text = (\n",
    "    \"나는 둥근형 얼굴이고, 직모에 굵은 모발을 가졌으며, \"\n",
    "    \"관리 난이도 쉬운 남성 스타일을 추천해줘.\"\n",
    ")\n",
    "\n",
    "param_local = QueryParam(mode=\"local\")\n",
    "response_local = graph_func.query(query_text, param_local)\n",
    "print(\"=== Local RAG Recommendation ===\")\n",
    "print(response_local)  # JSON 형식의 community_report_hair 결과\n",
    "\n",
    "param_global = QueryParam(mode=\"global\")\n",
    "response_global = graph_func.query(query_text, param_global)\n",
    "print(\"=== Global RAG Recommendation ===\")\n",
    "print(response_global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0067284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df= pd.read_csv('hair_data/for_graph.csv')\n",
    "df\n",
    "# 요약 부분에 마크다운 형식 즉 *,# 등이 있으면 삭제하는 코드\n",
    "import re\n",
    "def remove_markdown(text):\n",
    "    # 마크다운 형식의 문자를 정규 표현식으로 제거\n",
    "    text = re.sub(r'[*#]', '', text)\n",
    "    return text\n",
    "df['요약'] = df['요약'].apply(remove_markdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce4f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:[New Docs] inserting 10 docs\n",
      "INFO:nano-graphrag:[New Chunks] inserting 10 chunks\n",
      "INFO:nano-graphrag:[Entity Extraction]...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1(10%) chunks,  10 entities(duplicated), 8 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Processed 2(20%) chunks,  21 entities(duplicated), 18 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ Processed 3(30%) chunks,  44 entities(duplicated), 34 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠼ Processed 4(40%) chunks,  62 entities(duplicated), 48 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠴ Processed 5(50%) chunks,  85 entities(duplicated), 67 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠦ Processed 6(60%) chunks,  106 entities(duplicated), 87 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠧ Processed 7(70%) chunks,  129 entities(duplicated), 107 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠇ Processed 8(80%) chunks,  150 entities(duplicated), 127 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠏ Processed 9(90%) chunks,  190 entities(duplicated), 151 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ Processed 10(100%) chunks,  746 entities(duplicated), 161 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Inserting 724 vectors to entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.04it/s]\n",
      "INFO:nano-graphrag:[Community Report]...\n",
      "INFO:nano-graphrag:Each level has communities: {0: 9, 1: 13}\n",
      "INFO:nano-graphrag:Generating by levels: [1, 0]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Processed 2 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ Processed 3 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠼ Processed 4 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠴ Processed 5 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠦ Processed 6 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠧ Processed 7 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠇ Processed 8 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠏ Processed 9 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ Processed 10 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 11 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Processed 12 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ Processed 13 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠼ Processed 14 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠴ Processed 15 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠦ Processed 16 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠧ Processed 17 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠇ Processed 18 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠏ Processed 19 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ Processed 20 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 21 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:nano-graphrag:JSON data successfully extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Processed 22 communities\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-graphrag:Writing graph with 732 nodes, 161 edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = df[\"요약\"].dropna().astype(str).tolist()  # 🔥 행 하나 = 하나의 청크로 사용됨\n",
    "graph_func.insert(docs[25:35])\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c3b0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.03it/s]\n",
      "INFO:nano-graphrag:Using 20 entites, 1 communities, 18 relations, 7 text units\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 2) Local RAG 모드\u001b[39;00m\n\u001b[1;32m     10\u001b[0m param_local \u001b[38;5;241m=\u001b[39m QueryParam(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m response_local \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_local\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Local RAG Recommendation ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_local)  \u001b[38;5;66;03m# JSON 형식의 community_report_hair 결과\u001b[39;00m\n",
      "File \u001b[0;32m~/AIworkspace/poject/DSCD-1/nano-graphrag/nano_graphrag/graphrag.py:224\u001b[0m, in \u001b[0;36mGraphRAG.query\u001b[0;34m(self, query, param)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, param: QueryParam \u001b[38;5;241m=\u001b[39m QueryParam()):\n\u001b[1;32m    223\u001b[0m     loop \u001b[38;5;241m=\u001b[39m always_get_an_event_loop()\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.10/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.10/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m/opt/anaconda3/envs/graphrag_env/lib/python3.10/selectors.py:562\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from nano_graphrag import GraphRAG, QueryParam  # QueryParam 별도 import\n",
    "\n",
    "# 1) 질의 내용 정의\n",
    "query_text = (\n",
    "    \"나는 둥근형 얼굴이고, 직모에 굵은 모발을 가졌으며, \"\n",
    "    \"관리 난이도 쉬운 남성 스타일을 추천해줘.\"\n",
    ")\n",
    "\n",
    "# 2) Local RAG 모드\n",
    "param_local = QueryParam(mode=\"local\")\n",
    "response_local = graph_func.query(query_text, param_local)\n",
    "print(\"=== Local RAG Recommendation ===\")\n",
    "print(response_local)  # JSON 형식의 community_report_hair 결과\n",
    "\n",
    "# 3) Global RAG 모드\n",
    "param_global = QueryParam(mode=\"global\")\n",
    "response_global = graph_func.query(query_text, param_global)\n",
    "print(\"=== Global RAG Recommendation ===\")\n",
    "print(response_global)\n",
    "\n",
    "# 4) (Optional) Naive 모드\n",
    "# param_naive = QueryParam(mode=\"naive\")\n",
    "# response_naive = graph_func.query(query_text, param_naive)\n",
    "# print(\"=== Naive RAG Recommendation ===\")\n",
    "# print(response_naive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436d54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
